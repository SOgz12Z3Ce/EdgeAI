# 监督学习与无监督学习

## 异同

监督学习（Supervised Learning）与无监督学习（Unsupervised Learning）都是机器学习方法。听名字似乎很有关系，但研究的问题差别非常大。粗糙地来看，监督学习从**数据**和它们的**标签**中学习，目的是得到一个可以**预测**或者说**推理**的模型。无监督学习直接从**数据**中学习，目的是揭示数据内部结构。

这两种学习方式可以处理的问题当然也是不同的。监督学习面对的任务一般是**分类**与**回归**，抽象地来看，它们都要拟合一个输入和输出都是向量的函数，但是实现上的各种差别很大，以至于将这两种任务分开讨论是有好处的。无监督学习可以处理的任务很多，比较耳熟能详的有**聚类**和**降维**。

尽管两者差异很大，原理仍然有相似之处：

- 它们都是“优化问题”，都要让一个评估函数优化到最值。
- 都需要基于一定的数学模型和算法，在现在常用各类神经网络与梯度下降。

## 监督学习

“监督学习”一词的“监督”指的是数据带有标签，这些标签会指导模型学习，不断调整自己的行为。

一个监督学习方法有这些基本部分：

- 模型，将输入映射到输出的方法；
- 损失函数，评估准确度；
- 优化器，调整参数的方法。

下面记录了几种经典的监督学习方法。

### 线性回归

线性回归（Linear Regression）可能是我们最早学到的监督学习方法了。它适用于回归问题。模型很简单，就是对输入作线性变换：

$$
y = w \cdot x + b
$$

损失函数一般使用均方误差：

$$
\frac{1}{m} \sum_{i=1}^m (\hat{y_i} - y_i)^2
$$

我们其实已经知道这就是最小二乘法。高中告诉我们它可以直接求解析解，不过工程上也可以用梯度下降法作为优化器。

线性回归有一个值得注意的地方，就是它只作一次变换，没有分层。因为多个线性变换可以表示为一个，所以分层没啥意义。以后有时会用到这一点。

### 逻辑回归

逻辑回归（Logistic Regression）与线性回归有些像，只不过对结果加了一层 sigmoid。它适用于分类，虽然名字里有个回归。

模型很简单：

$$
z = w \cdot x + b
$$

$$
\hat{p} = \sigma(z) = \frac{1}{1 + e^{-z}}
$$

由于 sigmoid 的性质，结果 \hat{p} 会落在 (0, 1) 间，可以被解释为分类的概率。

逻辑回归使用交叉熵损失函数：

$$
\text{L} = y \log \hat{p} + (1-y) \log(1-\hat{p})
$$

这个函数是由最大似然估计得出的，它的特点是预测得不对时，\text{L} 增长得非常快。不使用均方误差是因为那会让问题变成非凸的，而凸优化问题的性质更好，局部最优就是全局最优。另外，sigmoid 和交叉熵同时使用可以使得求导变得很简单：

$$
\frac{\partial L}{\partial z} = \sigma(z) - y
$$

逻辑回归是凸的，所以可以用梯度下降作为优化器。

### 支持向量机

支持向量机（Support Vector Machine，SVM）可能是非深度学习的方法中最具代表性的一个。它用于分类。

数据首先被表示为高维空间中的一个点，这个空间就是**特征空间**。一个**超平面**可以把不同类的点分开，超平面是比特征空间低一维的子空间，例如二维空间里的线、三位空间里的面。

支持向量机想要找到**最优超平面**，也就是说距离超平面最近的两种样本点的距离之和最大的超平面，这个距离之和叫做**间隔**，这些最近的样本点就是**支持向量**。

现实世界中的问题一般不理想，数据在特征空间中不一定可以被超平面分开，或者由于误差，硬间隔的支持向量机可能太死板。支持向量机有很多扩展，**软间隔**设计了损失函数来允许分类犯错。升维也可以让不可分的数据变为可分的，而**核方法**在此过程中可以绕过显式的升维操作。

在使用支持向量机预测时，只需计算它相对于最优超平面的位置：

$$
\hat{y} = \operatorname{sign}( w \cdot x + b)
$$

硬间隔的支持向量机不需要损失函数，但是软间隔的损失函数是：

$$
\text{L} = \frac{1}{2}\|w\|^2 + C \sum_{i=1}^n \max(0,\ 1 - y_i(w \cdot x_i + b))
$$

其中 C 是一个权衡参数，第一项是正则化。

y_i 与 w \cdot x_i + b 的符号在分类正确时相同，错误时相反，所以当分类正确且离最优超平面足够远时，这个点不会产生损失值；当离得比较近或者分类错误时，损失值增大。

支持向量机是凸的，所以可以用梯度下降作为优化器。然而由于它的性质很好，也有数学方法（对偶问题 + 拉格朗日乘子）来求解。

## 无监督学习

无监督学习听起来很有魅力，它可能发现我们未曾发现的数据内在结构。尽管方法各有不同，但最终都可以为我们提供一个观察数据的新视角。

### K-means 聚类

在聚类算法中，常常称不同类的数据点为**簇**（cluster）。可以将簇中的数据点求均值，得出这个簇的**中心**（centroids）。

“K-means”的“mean”就是均值的意思，“K”是参数，表示簇的数量。

K-means 聚类会首先选取 K 个点为初始中心，然后将离中心最近的点划为一个簇，计算新的中心，如此反复直到中心位置几乎不变或者运行次数足够多。

K-means 聚类的原理非常直白，但是它有一些明显的缺点，比如对长得不像球形的簇效果不好，例如，圆环、长条、月牙都不容易被正确聚类。
