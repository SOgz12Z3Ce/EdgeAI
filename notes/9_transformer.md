# Transformer

> Attention Is All You Need!

俗话说字越少事越大，Transformer 在《Attention Is All You Need》中被提出以来，这句话已经耳熟能详了。现在人人都在用包含 Transformer 的各种 AI 工具，让我看看怎么个事。

## 历史

### 循环神经网络

故事从循环神经网络（Recurrent neural network，RNN）开始。许多网络都要求输入是给定形状的张量，并输出特定形状的张量，这在输入长度不能确定的任务（如文本翻译）中天然地不适应。循环神经网络可以解决这种问题。

循环神经网络可以拥有不定数量的隐藏层，当然了，这些隐藏层的参数不能凭空变出来，它们其实会共享参数。每层隐藏层的输出被称为**隐藏状态**，隐藏状态依赖上一个隐藏状态和输入，而第一个隐藏状态则依赖一个**初始隐藏状态**，可以是全零也可以可训练。隐藏状态可以被使用其他网络“解码”成有意义的输出。

从循环神经网络的流程看来，它的想要把每一个输入“压缩”到隐藏状态中，让信息聚合到一起。

循环神经网络的不足在于，随着输入序列变长，网络越来越深，而深的网络更容易梯度消失或爆炸，不容易训练。

### 长短期记忆网络

长短期记忆网络（Long Short-Term Memory，LSTM）为了避免 RNN 那样的梯度爆炸和消失问题改进了网络结构。

LSTM 有**短期记忆（hidden state）**，用于存储小范围上下文的信息。它在形式上和 RNN 中的隐藏状态似乎比较像，但是它保存的信息从全文变成了仅上下文，这有助于网络更好地关注和处理局部。LSTM 还加入了**长期记忆（cell state）**，它像是一条主线，用于存储很久之前的信息。

在一轮时间步中，上一轮的短期记忆和这一轮输入会经线性变换和 sigmoid 算出一个值，来决定长短期记忆的更改强度，它们被称为各种“门”：

- 遗忘门：长期记忆应保留的百分比。
- 输入门：长期记忆需要增加的百分比，与长期候选记忆一起使用。
- 输出门：下一轮短期记忆的百分比，与短期候选记忆[^transformer_short]一起使用。

[^transformer_short]: 其实就是更新后的长期记忆经过 tanh，有的资料不会单独拎出来。

长期记忆先与遗忘门相乘[^transformer_muti]，再加长期候选记忆乘输入门。更新后的长期记忆的 tanh 就是短期候选记忆。新的短期记忆就是短期候选记忆乘输出门。对比一下，长期记忆是在原先的基础上更新的，短期记忆则直接算了一个新的出来，这也许能解释为啥它们叫长期记忆和短期记忆。

[^transformer_muti]: 指逐元素乘，下同。

回头看看这个网络结构，虽然用到了很多导数小于一的激活函数，但长期记忆本身不会经过激活函数（或者说，经过的是恒等变换）或者自由的权重并且使用加法更新，其实不容易梯度消失。而且导数值不会大于一，所以更不会梯度爆炸。

下图展示了 LSTM 的结构：

![LSTM Figure1](./images/lstm_figure_0.png)

左上角的结构中，每种颜色的圈为一组，表示输入、隐藏状态、输出向量，每个圈是向量的某一维，计算顺序是从下往上的。注意表示隐藏状态的黄色圈被一条红线贯穿，这条红线就是长期记忆，在每一个时序运算完后算出下一时序的隐藏状态。

左下角把向量表示为一个圆圈，绿色框中从左到右展示了遗忘门、输入门、长期候选记忆、输出门和计算新的短期记忆的方法。从上方引出的一支表示每一层的短期记忆就是这一层的输出。

右侧展示了训练数据的结构，竖着的每一列就是一个输入向量。

下面的图表达的意思差不多，使用箭头注明了两种示意图的对应关系。

![LSTM Figure2](./images/lstm_figure_1.png)

### 词嵌入 / 文本向量化

词嵌入（Word Embedding）和文本向量化（Text Vectorization）都是将文本表示成向量的方法，后者包含前者，指的是任意文本而不是局限于词语的向量化。我们熟知 one-hot 向量可以编码词汇，但是这种编码完全没有体现出词语之间的联系，所有词都是一致的，不同就是不同。这其实丢失了一些语言本身的信息，而且维度太大不好训练。词嵌入和文本向量化强调了词语之间的联系，同时有效减少了维度。

**Word2Vec**是一种机器学习方法，用于实现词嵌入。

我喜欢想象文本向量化后，词嵌入空间里面的每个点就表示了一个“所指”，也就是一种具体含义。所以文本向量化后：

- 向量会分布在各个簇中，意思相近的词对应的向量在各种方面来看都相似，反之亦然。
- 向量可加，可以把词向量相加来拼凑含义：“墨索里尼” - “披萨” + “烤肠”的结果离“希特勒”很近。（听说的）

还有一个值得注意的事情：token 不是词，标点符号也有词嵌入。标点符号也有语义，这倒是不难理解（

#### 高维向量的相似度

既然文本向量化强调了词语之间的联系，那如何体现出来呢？直觉上来看，两个意思相近的词应该在向量化后也变成相似的向量，所以可以衡量向量的相似度：

- 距离：计算差向量长度，越小越近，向量就越相似。
- 余弦相似度：计算向量夹角余弦值，值越大，向量的方向越接近，向量就越相似。
  - 还能直接算点积。
- Mahalanobis 距离。

研究高维向量的相似度可以把语言学问题变成数学问题，或者说，这是为语言建模的一部分。

### Encoder-Decoder / Seq2Seq

Transformer 本来就是解决**序列到序列（Seq2Seq）**的模型，“序列”指的是像文本、音频这样有序的数据，翻译文本、语音识别等工作就是 Seq2Seq 的。

《Attention Is All You Need》开篇就提到了传统的 Seq2Seq 模型：常使用一个 **Encoder** 来读取输入，编码成中间表示，然后把这个中间表示输出给一个 **Decoder** 来解释。这其实就是 Encoder-Decoder 结构和原理，Encoder 和 Decoder 的实现方式不限定，常见的有两种：

- RNN，这里的中间表示就是隐藏向量。
  - 常用于 Seq2Seq。
  - 有时会在中间多加一个 Attention 层。
- CNN，这里的中间表示是特征图。
  - 用于图像，一般不用于 Seq2Seq。

Seq2Seq 常用的 RNN 在训练时有些特点：

- 它名字上是循环的，但计算图其实还是无环的，训练时需要展开这个图。
- Decoder 是**自回归的**，它生成下一段序列时依赖自己生成的上一段序列，训练时要每一步都强行调成正确的，不能一次生成完了再检查。

#### Attention

Attention 机制要解决的问题就是 RNN 在太长的上下文中容易丢失前文信息，毕竟就凭一个中间表示就想把所有信息无损压缩进去，还是不太实际。Attention 在 Decoder 计算时根据输入序列分配权重，权重的大小取决于输入与当前解码的隐藏状态相关性，也就是相似度。

比如点积注意力：

- 先计算输入隐藏状态（$h_i$）与解码器隐藏状态（$s_t$）的相似度：$score(s_t, h_i) = s_t^\top h_i$
- 取 softmax，得到 $\alpha_{t,i}$，这就是权重
- 计算上下文向量：$c_t = \sum_i \alpha_{t,i} h_i$

最后得到的 $c_t$ 就包含了上下文信息，它可以和 Decoder 的隐藏状态一起用，给出跟好的输出。

Attention 机制不止这一种，同样是在 Decoder 时注意到前文信息，还有：

- 多头注意力：拆分 Attention，每个 Attention 可以理解成“注意到不同的方面”
- 自注意力：同一段文本自己对自己注意。

## Attention Is All You Need

《Attention Is All You Need》给出了新的架构，它还是由 Encoder 和 Decoder 组成[^transformer_decoderonly]，完全抛弃了传统的 RNN 路线，取而代之的是大量的多头注意力（点题！）和前馈神经网络。

[^transformer_decoderonly]: 不过后来大家又尝试去掉 Encoder，改为完全自回归的 Decoder-only 结构了，所以论文原文给出的架构看起来就和某些科普里的不太一样。

### 输入

在 Encoder，输入首先被 tokenizer 分割并映射为词嵌入向量，而 Decoder 是自回归的，每一轮输出会作为输入扔进 Decoder，第一次输出时会输入一个约定的特殊字符表示输出开始。由于抛弃了 RNN，此时天然看不出不同位置的同一个词有什么区别，所以需要加一个位置编码给词嵌入向量：

$$
PE(pos, 2i) = \sin\left( \frac{pos}{10000^{2i / d}} \right)

PE(pos, 2i + 1) = \cos\left( \frac{pos}{10000^{2i / d}} \right)
$$

取这两个函数有一些数学上的好处，比如 $PE(pos, k)$ 可以由 $PE(pos)$ 线性表出（三角恒等变换），这可以强调相对位置关系。不过这个东西后来被发现也可以是可学习的参数。

### 多头注意力

随后向量进入多头注意力（自注意的）[^transformer_layernorm]。这里的注意力目的和上面的不一样，它是为了让词与词之间交换信息，丰富语义用的。为了理解，考虑“你好”和“叶公好龙”里面的“好”字，它们意思肯定不一样，但是 embedding 却只能嵌入成一样的向量，所以需要额外的上下文信息。

[^transformer_layernorm]: 这里原文似乎没有做层归一，不过后来者一般要做一次层归一。

注意力模块的参数包含三个大矩阵，分别是 Q(ueue)、K(ey)、V(alue)，注意它们作为参数要先和输入作矩阵乘，才是公式里面的 QKV。我们可以从公式里感受一下它们的语义：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left( \frac{Q K^\top}{\sqrt{d_k}} \right) V
$$

把 Q 和 K 分块，每一列都是一个向量，那么分子上的 $QK^T$ 就表示这些词向量互相作点积的结果矩阵，值越大，两个向量就越相似，也就是说 Q 里面的向量**查询**到了 K 里的向量。这个值还要整体缩放，除以 $\sqrt{d_k}$，这是出于工程实践的做法，因为 $QK^T$ 值比较大，避免梯度消失。随后 softmax 表示为对 V 加权和的权重。

这个过程中，Q 会“查询”（我更喜欢说“匹配”）到了 K，那么 K 对应的 V 会被累加到最终值上面去。这也是 Key、Value 这种命名的来历，就像平常的 `dict` 按索引取值一样，不过在这里不需要 Key 精确匹配了，而是匹配了多大程度就取多少值。

上面只是一个注意力层，多头注意力一般会把 QKV 矩阵切小，然后把输出拼到一起。这里切的尺寸虽然可以是任意的，但一般都选择与单个注意力层参数量一致的均分，处理起来会方便。

输出矩阵随后经一个线性层映射，多头注意力层到此就结束了。

在 Decoder 中，输入部分的多头注意力还是加了一层**掩码**的，阻止后面的词注意到前面的词。这本身倒是好理解，预测文本时后面的词不可以影响到前面。不过这有一个附带的好处：训练时后面的输入和前面的完全无关，这使得它可并行了。

另外，Encoder 的输出不会像传统的那样形成一个中间表达传递给 Decoder，Decoder 在生成时会让 Encoder 的输出参与到多头注意力中作为 Q 和 K，这叫做**交叉注意力**。

### 残差连接与层归一化

Transformer 里有很多“Add & Norm”，这个操作是为了缓解梯度消失和爆炸。

上一层网络的输出不是下一层网络的输入，而是需要和原始输入相加。换句话说，网络学习的不是输入到输出的直接映射，而是输入和输出之间的**残差**，这种设计叫做**残差连接**。这让人想起 STML 里面的长期记忆。

**层归一化**则是一个常见套路，它保留输出的分布，却又要把它们控制在合理的值范围内。层归一化有两步：

- 将输出减去它所有特征的均值，除以它所有特征的方差加上一个小值（避免除零）的和的平方根。这样可以让它所有特征均值为 0，方差为 1。
- 对每种特征做一次缩放和偏移（参数可学习），避免过分限制模型的取值。

在原文中，残差连接与层归一化是先后完成的，但是这个顺序也可以反过来。

#### 批量归一化

还有个东西叫批量归一化（Batch Normlization），它对所有样本的某一特征作归一化，计算方法基本不变。由于 Transformer 处理的是会变长的问题，批量归一化不稳定，所以都用层归一化。

### 前馈层

前馈层很简单，一层线性、一个 ReLU、再一层线性。这就是添加非线性用的。不过，虽然它描述起来很简单，却几乎占了 2/3 的模型参数量，这可能暗示了模型的训练时的重心。

### 输出

输出层是一个线性层加上 Softmax，这个 Softmax 的输出最后被当成概率来看待了，语言模型的参数里有一项“温度”，也是用在这里的。
